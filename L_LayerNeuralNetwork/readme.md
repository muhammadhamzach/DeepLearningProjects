# L Layer Neural Network with Regularization & Gradient Descent Tuning

* One dataset is provided
* main.py is used to run this code. 
* User can now implement any number of layers with any number of hidden neurons in each layer
* Activation function (relu & sigmoid) are implemented
* Regularization (both lambda and drop out) are also implemented
* Gradient Descent (Batch Gradient Descent & Mini-Batch Gradient Descent) also implemented
* Mini-Batch Gradient Descent Optimization (Regular, Momentum, Adam) also implemented

### Note
* The code is purely written in python with all its Neural Functions implemented manually instead of using pre-built Nerual libraries

## Reference
Code is modified from the deeplearning.ai Neural Networks & Hyper parameters course
