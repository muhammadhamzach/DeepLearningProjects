# L-Layer Neural Network Using Tensorflow

* dataset is provided which is automatically loaded up on startup
* main.py is used to execue the scripts
* Softmax/Relu Function pair is used to calculate the output
* The number of layers and no of neurons in each layer is decided by user
* Used Xavier Initilization of parameters
* Can use both Gradient Descent as "gd" & Adam Optimization as "adam" (including its params beta1, beta2, optimizer) as an update optimization parameter rule
* Learning Rate, No of Iterations and Mini-Batch/Full Batch Gradient Descent can be user selected

## Notebook
The code is also implemented in Google Collab and can be used instead of individual scripts too


## Note
* This script is backward compatible to Tensorflow v1.x

## Reference
* This script is derived from the deeplearning.ai Hyper-parameters course 

